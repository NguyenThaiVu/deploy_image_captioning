{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "In this notebook, we will train the Image Captioning model:\n",
    "- Dataset: Flickr8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FILE_CAPTION = 'data/captions.txt'\n",
    "PATH_FOLDER_IMAGES = 'data/Images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the captions\n",
    "df = pd.read_csv(PATH_FILE_CAPTION)\n",
    "\n",
    "# Building the vocab\n",
    "vocab = Vocabulary(freq_threshold=1)\n",
    "\n",
    "vocab.build_vocab(df.caption.values)\n",
    "print(f\"Number of words in the vocab: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Custom Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((226, 226)),\n",
    "    transforms.RandomCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = FlickrDataset(df, vocab, path_folder_images=PATH_FOLDER_IMAGES, transform=transform)\n",
    "\n",
    "display_random_image(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Prepare the Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATH_SIZE = 128\n",
    "NUM_WORKERS = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "padding_value = dataset.vocab.str_2_int['<PAD>']\n",
    "data_loader = DataLoader(dataset, batch_size=BATH_SIZE, num_workers=NUM_WORKERS, shuffle=True, collate_fn=Collate(padding_value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define model architecture\n",
    "\n",
    "- Model: seq2seq model. \n",
    "- Encoder: pretrained Mobile v3 model. \n",
    "- Decoder: Bahdanau Attention and LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        base_model = models.mobilenet_v3_small()\n",
    "        for param in base_model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        modules = list(base_model.children())[:-2]\n",
    "        self.base_model = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.base_model(images)                                    # (batch_size, 576, 7, 7)\n",
    "        features = features.permute(0, 2, 3, 1)                           # (batch_size, 7, 7, 576)\n",
    "        features = features.view(features.size(0), -1, features.size(-1)) # (batch_size, 49, 576)\n",
    "        return features\n",
    "    \n",
    "    \n",
    "def calculate_number_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = torch.zeros((32, 3, 224, 224))\n",
    "encoder = EncoderCNN()\n",
    "test_output_encoder = encoder(test_image)\n",
    "\n",
    "print(f\"Test Output Shape: {test_output_encoder.shape}\")\n",
    "print(f\"Number of parameters in the model: {calculate_number_parameters(encoder)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bahdanau Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        self.W = nn.Linear(decoder_dim,attention_dim)\n",
    "        self.U = nn.Linear(encoder_dim,attention_dim)\n",
    "        \n",
    "        self.A = nn.Linear(attention_dim,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, features, hidden_state):\n",
    "        u_hs = self.U(features)     #(batch_size,num_layers,attention_dim)\n",
    "        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n",
    "        \n",
    "        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim)\n",
    "        \n",
    "        attention_scores = self.A(combined_states)         #(batch_size,num_layers,1)\n",
    "        attention_scores = attention_scores.squeeze(2)     #(batch_size,num_layers)\n",
    "        \n",
    "        \n",
    "        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,num_layers)\n",
    "        \n",
    "        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,num_layers,features_dim)\n",
    "        attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n",
    "        \n",
    "        return alpha,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention Decoder\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        #save the model param\n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention_dim = attention_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
    "        \n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.lstm_cell = nn.LSTMCell(embed_size + encoder_dim, decoder_dim, bias=True)\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
    "        \n",
    "        self.fcn = nn.Linear(decoder_dim, vocab_size)\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "        \n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \n",
    "        embeds = self.embedding(captions)\n",
    "        \n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
    "        \n",
    "        # get the seq length to iterate\n",
    "        seq_length = len(captions[0]) - 1 # Exclude the last one\n",
    "        batch_size = captions.size(0)\n",
    "        num_features = features.size(1)\n",
    "        \n",
    "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, seq_length, num_features).to(device)\n",
    "                \n",
    "        for s in range(seq_length):\n",
    "            alpha, context = self.attention(features, h)\n",
    "            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "                    \n",
    "            output = self.fcn(self.drop(h))\n",
    "            \n",
    "            preds[:,s] = output\n",
    "            alphas[:,s] = alpha  \n",
    "        \n",
    "        return preds, alphas\n",
    "    \n",
    "    def generate_caption(self, features, max_len=20, vocab=None):\n",
    "        # Inference part: given the image features generate the captions\n",
    "        \n",
    "        batch_size = features.size(0)\n",
    "        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n",
    "        \n",
    "        alphas = []\n",
    "        \n",
    "        # starting input\n",
    "        word = torch.tensor(vocab.str_2_int['<SOS>']).view(1,-1).to(device)\n",
    "        embeds = self.embedding(word)\n",
    "\n",
    "        captions = []\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            alpha, context = self.attention(features, h)\n",
    "            \n",
    "            # store the apla score\n",
    "            alphas.append(alpha.cpu().detach().numpy())\n",
    "            \n",
    "            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n",
    "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
    "            output = self.fcn(self.drop(h))\n",
    "            output = output.view(batch_size,-1)\n",
    "        \n",
    "            # select the word with most val\n",
    "            predicted_word_idx = output.argmax(dim=1)\n",
    "            \n",
    "            # save the generated word\n",
    "            captions.append(predicted_word_idx.item())\n",
    "            \n",
    "            # end if <EOS detected>\n",
    "            if vocab.int_2_str[predicted_word_idx.item()] == \"<EOS>\":\n",
    "                break\n",
    "            \n",
    "            # send generated word as the next caption\n",
    "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
    "        \n",
    "        # covert the vocab idx to words and return sentence\n",
    "        return [vocab.int_2_str[idx] for idx in captions], alphas\n",
    "    \n",
    "    \n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderRNN(embed_size=128, vocab_size=len(dataset.vocab), attention_dim=256, encoder_dim=576, decoder_dim=512)\n",
    "\n",
    "test_output_decoder, _ = decoder(test_output_encoder, torch.randint(0, 100, (32, 16)))\n",
    "\n",
    "print(f\"Test Output Shape: {test_output_decoder.shape}\")\n",
    "print(f\"Number of parameters in decoder: {calculate_number_parameters(decoder)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,embed_size, vocab_size, attention_dim, encoder_dim, decoder_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderCNN()\n",
    "        self.decoder = DecoderRNN(\n",
    "            embed_size=embed_size,\n",
    "            vocab_size = vocab_size,\n",
    "            attention_dim=attention_dim,\n",
    "            encoder_dim=encoder_dim,\n",
    "            decoder_dim=decoder_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "embed_size = 128\n",
    "vocab_size = len(dataset.vocab)\n",
    "attention_dim = 256\n",
    "encoder_dim = 576\n",
    "decoder_dim = 256\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Init model\n",
    "model = EncoderDecoder(\n",
    "    embed_size = embed_size,\n",
    "    vocab_size = vocab_size,\n",
    "    attention_dim = attention_dim,\n",
    "    encoder_dim = encoder_dim,\n",
    "    decoder_dim = decoder_dim\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.str_2_int[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of parameters in the model: {calculate_number_parameters(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,num_epochs):\n",
    "    \"\"\"\n",
    "    Helper function to save the model\n",
    "    \"\"\"\n",
    "    model_state = {\n",
    "        'num_epochs':num_epochs,\n",
    "        'embed_size':embed_size,\n",
    "        'vocab_size':len(dataset.vocab),\n",
    "        'attention_dim':attention_dim,\n",
    "        'encoder_dim':encoder_dim,\n",
    "        'decoder_dim':decoder_dim,\n",
    "        'state_dict':model.state_dict()\n",
    "    }\n",
    "\n",
    "    torch.save(model_state,'attention_model_state.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    \n",
    "    #unnormalize \n",
    "    img[0] = img[0] * 0.229\n",
    "    img[1] = img[1] * 0.224 \n",
    "    img[2] = img[2] * 0.225 \n",
    "    img[0] += 0.485 \n",
    "    img[1] += 0.456 \n",
    "    img[2] += 0.406\n",
    "    \n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    \n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "print_every = 3\n",
    "\n",
    "for epoch in range(1, num_epochs+1):   \n",
    "    for idx, (image, captions) in enumerate(iter(data_loader)):\n",
    "        image,captions = image.to(device),captions.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, attentions = model(image, captions)\n",
    "\n",
    "        # Calculate the batch loss without the <START> token.\n",
    "        targets = captions[:, 1:]\n",
    "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
    "        \n",
    "        # generate the caption\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dataiter = iter(data_loader)\n",
    "            img,_ = next(dataiter)\n",
    "            features = model.encoder(img[0:1].to(device))\n",
    "            caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n",
    "            caption = ' '.join(caps)\n",
    "            show_image(img[0],title=caption)\n",
    "            \n",
    "        model.train()\n",
    "        \n",
    "    # save the latest model\n",
    "    save_model(model, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
